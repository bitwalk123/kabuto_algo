import torch
import torch.nn as nn
import torch.optim as optim
import pandas as pd
import numpy as np
import os


## PPO-lite Agent
class PPOAgent:
    def __init__(self, state_dim, action_dim, lr=1e-4, gamma=0.99, clip_epsilon=0.2):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.gamma = gamma
        self.clip_epsilon = clip_epsilon

        # Actor-Critic Network
        self.actor = self.create_actor_network(state_dim, action_dim)
        self.critic = self.create_critic_network(state_dim)

        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr)
        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=lr)

    def create_actor_network(self, state_dim, action_dim):
        return nn.Sequential(
            nn.Linear(state_dim, 64),
            nn.ReLU(),
            nn.Linear(64, action_dim),
            nn.Softmax(dim=-1)
        )

    def create_critic_network(self, state_dim):
        return nn.Sequential(
            nn.Linear(state_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 1)
        )

    def get_action(self, state):
        state = torch.FloatTensor(state)
        action_probs = self.actor(state)
        # ç¢ºç‡ã«åŸºã¥ã„ã¦ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’é¸æŠ
        action = torch.multinomial(action_probs, 1).item()
        return action

    def learn(self, states, actions, rewards, next_states, dones):
        # ãƒãƒƒãƒå‡¦ç†ã®ãŸã‚ã®ãƒ†ãƒ³ã‚½ãƒ«åŒ–
        states = torch.FloatTensor(np.array(states))
        actions = torch.LongTensor(actions)
        rewards = torch.FloatTensor(rewards)
        next_states = torch.FloatTensor(np.array(next_states))
        dones = torch.FloatTensor(dones)

        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå€¤ã®è¨ˆç®—
        next_values = self.critic(next_states).detach()
        targets = rewards + self.gamma * next_values * (1 - dones)

        # ä¾¡å€¤é–¢æ•°ã®æå¤±è¨ˆç®—ã¨æ›´æ–°
        values = self.critic(states)
        critic_loss = nn.MSELoss()(values, targets.detach())
        self.critic_optimizer.zero_grad()
        critic_loss.backward()
        self.critic_optimizer.step()

        # PPOã®Actoræ›´æ–°ï¼ˆã“ã“ã§ã¯ç°¡ç•¥åŒ–ï¼‰
        # å®Ÿéš›ã®PPOã§ã¯ã€å¤ã„ãƒãƒªã‚·ãƒ¼ã¨æ–°ã—ã„ãƒãƒªã‚·ãƒ¼ã®æ¯”ç‡ã‚’è¨ˆç®—ã—ã€ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ã‚’è¡Œã„ã¾ã™
        # ç°¡æ˜“å®Ÿè£…ã®ãŸã‚ã€ã“ã“ã§ã¯Actorã®æå¤±ã‚’ä¾¡å€¤é–¢æ•°ã®ã‚¢ãƒ‰ãƒãƒ³ãƒ†ãƒ¼ã‚¸ã«åŸºã¥ã„ã¦è¨ˆç®—ã—ã¾ã™ã€‚
        with torch.no_grad():
            advantages = targets - values

        old_action_probs = self.actor(states).gather(1, actions.unsqueeze(1)).detach()
        new_action_probs = self.actor(states).gather(1, actions.unsqueeze(1))

        ratio = new_action_probs / old_action_probs

        surrogate1 = ratio * advantages
        surrogate2 = torch.clamp(ratio, 1 - self.clip_epsilon, 1 + self.clip_epsilon) * advantages

        actor_loss = -torch.min(surrogate1, surrogate2).mean()

        self.actor_optimizer.zero_grad()
        actor_loss.backward()
        self.actor_optimizer.step()

    def save_model(self, path):
        torch.save({
            'actor_state_dict': self.actor.state_dict(),
            'critic_state_dict': self.critic.state_dict()
        }, path)

    def load_model(self, path):
        checkpoint = torch.load(path)
        self.actor.load_state_dict(checkpoint['actor_state_dict'])
        self.critic.load_state_dict(checkpoint['critic_state_dict'])


## TradingSimulation
class TradingSimulation:
    def __init__(self, model_path="ppo_model.pth"):
        self.model_path = model_path
        self.state_dim = 2  # çŠ¶æ…‹ç©ºé–“: [ç›´å‰ã®ä¾¡æ ¼å·®, å‡ºæ¥é«˜ã®å¤‰åŒ–ç‡]
        self.action_dim = 3  # 3ã¤ã®è¡Œå‹•: 0=ä½•ã‚‚ã—ãªã„, 1=è²·ã„, 2=å£²ã‚Š

        # ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã¨åˆæœŸåŒ–
        self.agent = PPOAgent(self.state_dim, self.action_dim)
        if os.path.exists(self.model_path):
            try:
                self.agent.load_model(self.model_path)
                print(f"âœ… æ—¢å­˜ã®å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {self.model_path}")
            except Exception as e:
                print(f"âš ï¸ æ—¢å­˜ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ ({e})ã€‚æ–°ã—ã„ãƒ¢ãƒ‡ãƒ«ã‚’ç”Ÿæˆã—ã¾ã™ã€‚")
                os.remove(self.model_path)
                self.agent = PPOAgent(self.state_dim, self.action_dim)
                print("ğŸ†• æ–°ã—ã„å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã‚’ç”Ÿæˆã—ã¾ã—ãŸã€‚")
        else:
            print("ğŸ†• å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ãŒå­˜åœ¨ã—ãªã„ãŸã‚ã€æ–°ã—ã„ãƒ¢ãƒ‡ãƒ«ã‚’ç”Ÿæˆã—ã¾ã—ãŸã€‚")

        # ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–
        self.position = 0  # 0=ãƒã‚¸ã‚·ãƒ§ãƒ³ãªã—, 1=è²·ã„, -1=å£²ã‚Š
        self.entry_price = 0  # å»ºç‰ä¾¡æ ¼
        self.realized_profit = 0  # å®Ÿç¾æç›Š
        self.results = []

        # éå»ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä¿æŒï¼ˆçŠ¶æ…‹ã®ç”Ÿæˆç”¨ï¼‰
        self.last_price = 0
        self.last_volume = 0
        self.last_state = None
        self.last_action = None

    def add(self, ts, price, volume, force_close=False):
        """
        åˆ¥ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‹ã‚‰ãƒ†ã‚£ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’å—ã‘å–ã‚Šã€å£²è²·ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’æ±ºå®šã™ã‚‹
        """
        # æœ€åˆã®ãƒ†ã‚£ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ã®å ´åˆã€éå»ãƒ‡ãƒ¼ã‚¿ã¨çŠ¶æ…‹ã‚’åˆæœŸåŒ–
        if self.last_price == 0:
            self.last_price = price
            self.last_volume = volume
            return "åˆæœŸåŒ–"

        # çŠ¶æ…‹ï¼ˆStateï¼‰ã®ç”Ÿæˆ
        # price_change: ç›´å‰ã®æ ªä¾¡ã‹ã‚‰ã®å¤‰åŒ–ç‡
        price_change = (price - self.last_price) / self.last_price if self.last_price != 0 else 0

        # volume_change: å‡ºæ¥é«˜ã®å¢—åˆ†ã‚’å¯¾æ•°ã§æ­£è¦åŒ–
        # ç´¯è¨ˆå‡ºæ¥é«˜ãªã®ã§ã€å¢—åŠ åˆ†ã®ã¿è€ƒæ…®ã€‚ãƒ­ã‚°ã‚’å–ã‚‹ã“ã¨ã§æ¡æ•°ã‚’æŠ‘ãˆã‚‹
        volume_increment = volume - self.last_volume
        if volume_increment > 0:
            volume_change = np.log1p(volume_increment)  # log(1+x)ã§0ã®æ™‚ã‚‚å¯¾å¿œ
        else:
            volume_change = 0

        state = [price_change * 100, volume_change]  # å¤‰åŒ–ç‡ã‚’ãƒ‘ãƒ¼ã‚»ãƒ³ãƒˆã«ã™ã‚‹ãªã©èª¿æ•´å¯èƒ½

        # å ±é…¬ï¼ˆRewardï¼‰ã®è¨ˆç®—ã¨å­¦ç¿’
        reward = 0
        if self.last_state is not None and self.last_action is not None:
            # å‰å›ã®è¡Œå‹•ã«å¯¾ã™ã‚‹å ±é…¬ã‚’è¨ˆç®—
            if self.last_action == 1:  # è²·ã„ã®å ´åˆ
                reward = (price - self.entry_price) * self.position * 100  # å«ã¿ç›Š
            elif self.last_action == 2:  # å£²ã‚Š(ç©ºå£²ã‚Š)ã®å ´åˆ
                reward = (self.entry_price - price) * self.position * 100  # å«ã¿ç›Š

            # å«ã¿ç›Šã‚’ãã®ã¾ã¾å ±é…¬ã«ã™ã‚‹ã¨ã€å€¤ãŒå¤§ãããªã‚Šã™ãã‚‹ãŸã‚ã€æ­£è¦åŒ–ã¾ãŸã¯ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ãŒå¿…è¦
            reward = reward / 1000  # ä¾‹ï¼š1000å††ã‚ãŸã‚Šã®å«ã¿ç›Šã‚’å ±é…¬ã¨ã™ã‚‹

            # æ—¢å­˜ã®ãƒã‚¸ã‚·ãƒ§ãƒ³ãŒã‚ã‚Œã°ã€å«ã¿æç›Šã‚’å ±é…¬ã«åŠ ãˆã‚‹
            if self.position != 0:
                unrealized_profit = (price - self.entry_price) * self.position * 100
                reward += unrealized_profit / 100000  # å«ã¿ç›Šã®å ±é…¬ã‚’åŠ ç®—

            # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®çµ‚äº†åˆ¤å®š
            done = False
            self.agent.learn([self.last_state], [self.last_action], [reward], [state], [done])

        # å¼·åˆ¶è¿”æ¸ˆãƒ•ãƒ©ã‚°ã®å‡¦ç†
        if force_close and self.position != 0:
            self.close_position(price)
            action_desc = "å¼·åˆ¶è¿”æ¸ˆ"
            self.results.append({
                "Time": ts,
                "Price": price,
                "Action": action_desc,
                "Reward": reward,
                "Profit": self.realized_profit
            })
            self.position = 0
            self.entry_price = 0
            self.last_state = state
            self.last_action = 0  # å¼·åˆ¶è¿”æ¸ˆå¾Œã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã¯0(ä½•ã‚‚ã—ãªã„)ã¨ã—ã¦ãŠã
            return action_desc

        # PPOã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«è¡Œå‹•ã‚’æ±ºå®šã•ã›ã‚‹
        action = self.agent.get_action(state)

        # å£²è²·ãƒ­ã‚¸ãƒƒã‚¯
        action_desc = "ä½•ã‚‚ã—ãªã„"

        if action == 1 and self.position == 0:
            self.open_position(price, "buy")
            action_desc = "è²·ã„"
        elif action == 2 and self.position == 0:
            self.open_position(price, "sell")
            action_desc = "å£²ã‚Š"
        elif action == 1 and self.position == -1:
            self.close_position(price)
            action_desc = "è²·ã„è¿”æ¸ˆ"
        elif action == 2 and self.position == 1:
            self.close_position(price)
            action_desc = "å£²ã‚Šè¿”æ¸ˆ"

        # çµæœã‚’è¨˜éŒ²
        self.results.append({
            "Time": ts,
            "Price": price,
            "Action": action_desc,
            "Reward": reward,
            "Profit": self.realized_profit if action_desc.endswith("è¿”æ¸ˆ") else None
        })

        # çŠ¶æ…‹ã‚’æ›´æ–°
        self.last_price = price
        self.last_volume = volume
        self.last_state = state
        self.last_action = action

        return action_desc

    def open_position(self, price, side):
        self.entry_price = price
        self.position = 1 if side == "buy" else -1

    def close_position(self, price):
        if self.position == 1:  # è²·ã„ãƒã‚¸ã‚·ãƒ§ãƒ³ã‚’è¿”æ¸ˆ
            profit = (price - self.entry_price) * 100
        elif self.position == -1:  # å£²ã‚Šãƒã‚¸ã‚·ãƒ§ãƒ³ã‚’è¿”æ¸ˆ
            profit = (self.entry_price - price) * 100
        else:
            profit = 0

        self.realized_profit += profit
        self.position = 0

    def finalize(self):
        """
        ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³çµ‚äº†æ™‚ã«çµæœã‚’è¿”ã™
        """
        # æœ€çµ‚çŠ¶æ…‹ã®å­¦ç¿’
        if self.last_state is not None and self.last_action is not None:
            # æœ€çµ‚çš„ãªå ±é…¬ã®è¨ˆç®— (å«ã¿ç›Šãªã©ã‚’æ¸…ç®—)
            final_reward = 0
            if self.position != 0:
                final_reward += self.realized_profit  # å®Ÿç¾æç›Šã‚’æœ€çµ‚å ±é…¬ã«åŠ ãˆã‚‹

            self.agent.learn([self.last_state], [self.last_action], [final_reward], [self.last_state],
                             [True])  # done=Trueã§ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰çµ‚äº†ã‚’é€šçŸ¥

        # ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜
        self.agent.save_model(self.model_path)

        df_result = pd.DataFrame(self.results)
        # æ¬¡ã®Epochã®ãŸã‚ã«çŠ¶æ…‹ã‚’ãƒªã‚»ãƒƒãƒˆ
        self.results = []
        self.position = 0
        self.entry_price = 0
        self.realized_profit = 0
        self.last_price = 0
        self.last_volume = 0
        self.last_state = None
        self.last_action = None

        return df_result


## åˆ¥ãƒ—ãƒ­ã‚°ãƒ©ãƒ ï¼ˆãƒ¡ã‚¤ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼‰
if __name__ == "__main__":
    # å®Ÿéš›ã«ã¯ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’é©åˆ‡ã«è¨­å®šã—ã¦ãã ã•ã„
    # Windowsã§å‹•ãã“ã¨ã‚’è€ƒæ…®ã—ã¦ã€ãƒ‘ã‚¹åŒºåˆ‡ã‚Šæ–‡å­—ã¯os.path.joinã‚’ä½¿ã†ã®ãŒå®‰å…¨
    # Excelã®èª­ã¿è¾¼ã¿ã«ã¯openpyxlãŒå¿…è¦ã§ã™ï¼ˆpip install openpyxlï¼‰

    excel_file = "data/tick_20250818_7011.xlsx"
    epochs = 10

    # ãƒ†ã‚£ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€
    try:
        df = pd.read_excel(excel_file)
    except FileNotFoundError:
        print(f"ã‚¨ãƒ©ãƒ¼: æŒ‡å®šã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {excel_file}")
        exit()

    print(df)

    # ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ãƒ»ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
    sim = TradingSimulation()

    print("ãƒ†ã‚£ãƒƒã‚¯ãƒ•ã‚¡ã‚¤ãƒ«:", excel_file)
    # ç¹°ã‚Šè¿”ã—å­¦ç¿’
    for epoch in range(epochs):
        # 1è¡Œãšã¤ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã«æµã™
        for i, row in df.iterrows():
            ts = row["Time"]
            price = row["Price"]
            volume = row["Volume"]

            # æœ€å¾Œã®è¡Œã ã‘å¼·åˆ¶è¿”æ¸ˆãƒ•ãƒ©ã‚°ã‚’ç«‹ã¦ã‚‹
            force_close = (i == len(df) - 1)
            action = sim.add(ts, price, volume, force_close=force_close)
            # print(ts, price, action)

        # çµæœã‚’ä¿å­˜
        df_result = sim.finalize()
        total_profit = df_result["Profit"].sum() if "Profit" in df_result.columns and not df_result[
            "Profit"].isnull().all() else 0
        print(f"Epoch: {epoch}, ç·åç›Š: {total_profit}å††")
        df_result.to_csv(f"trade_results_{epoch}.csv")
